{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The US Department of State produces the *Foreign Relations of the United States (FRUS)* in both print and online forms. Their online volumes are housed in a Github for public use in TEI XML [here](https://github.com/HistoryAtState/frus). While the markup in the files depict useful identifiers such as the officials involved, the type of document, et cetera, this project requires plain text format rather than encoded documents. Using Oxygen XML Editor, I removed the markup and introductory information, leaving only the contents within the <text> tag. This creates a unique version of the files particular to my project. The XQuery removes the <teiHeader> which contains metadata, the front material which includes the introductory publication statements, and table of contents, and the back information such as the index. The query, which can be found [here] (https://drive.google.com/file/d/1MkVhOUxD4IJbdr4Hg0z7JN83tt5Sc9Se/view?usp=sharing), made each file into a plain text document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why word embeddings? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more info on word embeddings for the academic audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:1:13: unexpected '/'\n1: setwd(https:/\n                ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:1:13: unexpected '/'\n1: setwd(https:/\n                ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "setwd(https://github.com/ccloutier312/CloutierDHProject.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"tidyverse\")\n",
    "install.packages(\"tidytext\")\n",
    "install.packages(\"magrittr\")\n",
    "install.packages(\"devtools\")\n",
    "install.packages(\"tsne\")\n",
    "install.packages(\"usethis\")\n",
    "install.packages(\"SnowballC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidytext)\n",
    "library(magrittr)\n",
    "library(devtools)\n",
    "library(tsne)\n",
    "library(usethis)\n",
    "library(SnowballC)\n",
    "# Hold off on running the line below until after you get to the next section \n",
    "library(wordVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devtools::install_github('bmschmidt/wordVectors', force=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change \"name_of_your_folder\" to match the name of the folder with your corpus\n",
    "path2file <- \"data/frus/\"\n",
    "fileList <- list.files(path2file,full.names = TRUE) \n",
    "\n",
    "readTextFiles <- function(file) { # Remember that the code that defines functions must be run by putting your cursor at the beginning or end, or by selecting the whole section of code\n",
    "  message(file)\n",
    "  rawText = paste(scan(file, sep=\"\\n\",what=\"raw\",strip.white = TRUE))\n",
    "  output = tibble(filename=gsub(path2file,\"\",file),text=rawText) %>% \n",
    "    group_by(filename) %>% \n",
    "    summarise(text = paste(rawText, collapse = \" \"))\n",
    "  return(output)\n",
    "}\n",
    "\n",
    "combinedTexts <- tibble(filename=fileList) %>% \n",
    "  group_by(filename) %>% \n",
    "  do(readTextFiles(.$filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to change the text in the first line to whatever you want to call your model file\n",
    "baseFile <- \"your_file_name\"\n",
    "w2vInput <- paste(\"data/\",baseFile,\".txt\", sep = \"\")\n",
    "w2vCleaned <- paste(\"data/\",baseFile,\"_cleaned.txt\", sep=\"\")\n",
    "w2vBin <- paste(\"data/\",baseFile,\".bin\", sep=\"\")\n",
    "combinedTexts$text %>% write_lines(w2vInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model is based on the parameters that are selected prior to running the code. Word embedding models allow you to “choose how expansive you want the explored space to be” (Schmidt 2015). Tuning the parameters results in a greater accuracy depending on the analysis you are intending to complete. In order to test the usage of terms in the corpus, I tested a large variety of parameters in order to determine the accuracy of each model. For each set of parameters, both the Latin American and European corpora were tested and were run through six iterations on each corpus. Ultimately, there were six sets of parameters and therefore 62 models created in total. A list of the models and their parameters can be found here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THREADS <- 3\n",
    "\n",
    "prep_word2vec(origin=w2vInput,destination=w2vCleaned,lowercase=T,bundle_ngrams=1)\n",
    "\n",
    "#See the introductory file for a reminder on how you might adjust the parameters below\n",
    "if (!file.exists(w2vBin)) {\n",
    "  w2vModel <- train_word2vec(\n",
    "    w2vCleaned,\n",
    "    output_file=w2vBin,\n",
    "    vectors=500,\n",
    "    threads=THREADS,\n",
    "    window=10, iter=10, negative_samples=15\n",
    "  )\n",
    "} else {\n",
    "  w2vModel <- read.vectors(w2vBin)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to read in an existing file. Please see the Github repository for various iterations of this model that you can feed in. Be sure to remove the hashtag to choose this option instead.\n",
    "  #w2vModel <- read.vectors(\"data/test_for_cassie.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an image of the model and some exploration into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel %>% plot(perplexity=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel %>% closest_to(\"girl\", 30) %>% View()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel %>% closest_to(~\"girl\"+\"woman\"+\"girls\"+\"women\", 20) %>% View()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My thoughts on how these clusters show interesting patters of foreign policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers <- 150\n",
    "clustering <- kmeans(w2vModel,centers=centers,iter.max = 40)\n",
    "\n",
    "sapply(sample(1:centers, 10), function(n) {\n",
    "  names(clustering$cluster[clustering$cluster==n][1:15])\n",
    "})\n",
    "```\n",
    "\n",
    "#Change \"name_of_your_query\" to a descriptive name that you want to give to your export file.\n",
    "w2vExport <-sapply(sample(1:centers,150),function(n) {\n",
    "  names(clustering$cluster[clustering$cluster==n][1:15])\n",
    "})\n",
    "\n",
    "write.csv(file=\"output/name_of_your_query.csv\", x=w2vExport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original code includes a short method for validating the model. I have included five more word pairs that are good representations of the corpus. These were instrumental in determining which iterations of the model were the strongest representations of the corpus. You can find the Latin American validation data [here] (https://drive.google.com/file/d/1k75cYaVa8hmUtyoxOsi_0HX_mNmldD4B/view?usp=sharing) and the Europen validation data [here](https://drive.google.com/file/d/1oFj3rbjzaCtJgTvN0LvwPfVmyxIigJuo/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "You can run this test by hitting `command-return` or `control-return` to run one line a time, or just hit the green button in the top right of the code block below. \n",
    "\n",
    "files_list  = list.files(pattern=\"*.bin$\", recursive=TRUE)\n",
    "\n",
    "rownames <- c()\n",
    "\n",
    "data_frame <- data.frame()\n",
    "data = list(c(\"away\", \"off\"),\n",
    "            c(\"before\", \"after\"),\n",
    "            c(\"cause\", \"effects\"),\n",
    "            c(\"children\", \"parents\"),\n",
    "            c(\"come\", \"go\"),\n",
    "            c(\"day\", \"night\"),\n",
    "            c(\"first\", \"second\"),\n",
    "            c(\"good\", \"bad\"),\n",
    "            c(\"last\", \"first\"),\n",
    "            c(\"kind\", \"sort\"),\n",
    "            c(\"leave\", \"quit\"),\n",
    "            c(\"life\", \"death\"),\n",
    "            c(\"girl\", \"boy\"),\n",
    "            c(\"little\", \"small\"),\n",
    "            c(\"oil\", \"petroleum\"),\n",
    "            c(\"state\", \"department\"),\n",
    "            c(\"confidential\", \"secret\"),\n",
    "            c(\"east\", \"west\"),\n",
    "            c(\"aid\", \"assistance\"))\n",
    "\n",
    "\n",
    "data_list = list()\n",
    "\n",
    "for(fn in files_list) {\n",
    "  \n",
    "  wwp_model = read.vectors(fn)\n",
    "  sims <- c()\n",
    "  for(pairs in data)\n",
    "  {\n",
    "    vector1 <- c()\n",
    "    for(x in wwp_model[[pairs[1]]]) {\n",
    "      vector1 <- c(vector1, x)\n",
    "    }\n",
    "    \n",
    "    vector2 <- c()\n",
    "    for(x in wwp_model[[pairs[2]]]) {\n",
    "      vector2 <- c(vector2, x)\n",
    "    }\n",
    "    \n",
    "    sims <- c(sims, cosine(vector1, vector2))\n",
    "    f_name <- strsplit(fn, \"/\")[[1]][[2]]\n",
    "    data_list[[f_name]] <- sims\n",
    "  }\n",
    "  \n",
    "}\n",
    "\n",
    "for(pairs in data) {\n",
    "  rownames <- c(rownames, paste(pairs[1], pairs[2], sep=\"-\"))\n",
    "}\n",
    "\n",
    "results <- structure(data_list,\n",
    "                     class     = \"data.frame\",\n",
    "                     row.names = rownames\n",
    ")\n",
    "\n",
    "write.csv(file=\"output/model-test-results.csv\", x=results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
