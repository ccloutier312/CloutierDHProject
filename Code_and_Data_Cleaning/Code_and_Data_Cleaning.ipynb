{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The US Department of State produces the *Foreign Relations of the United States (FRUS)* in both print and online forms. Their online volumes are housed in a Github for public use in TEI XML [here](https://github.com/HistoryAtState/frus). While the markup in the files depict useful identifiers such as the officials involved, the type of document, et cetera, this project requires plain text format rather than encoded documents. Using an XQuery in Oxygen XML Editor, I removed the markup and introductory information, leaving only the contents within the <text> tag. This creates a unique version of the files particular to my project. The XQuery removes the <teiHeader> which contains metadata, the front material which includes the introductory publication statements, and table of contents, and the back information such as the index. The query, which can be found [here] (https://drive.google.com/file/d/1MkVhOUxD4IJbdr4Hg0z7JN83tt5Sc9Se/view?usp=sharing), made each file into a plain text document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, make sure that the you are in the correct location. Run the next chunk of code to ensure that the file path ends with \"CloutierDHProject/Methods and Code\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'/Users/cassie/Documents/GitHub/CloutierDHProject/Methods and Code'"
      ],
      "text/latex": [
       "'/Users/cassie/Documents/GitHub/CloutierDHProject/Methods and Code'"
      ],
      "text/markdown": [
       "'/Users/cassie/Documents/GitHub/CloutierDHProject/Methods and Code'"
      ],
      "text/plain": [
       "[1] \"/Users/cassie/Documents/GitHub/CloutierDHProject/Methods and Code\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "getwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The downloaded binary packages are in\n",
      "\t/var/folders/kw/rp8_72k54n5316nblkqcy8vr0000gq/T//RtmpqTTFas/downloaded_packages\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\t/var/folders/kw/rp8_72k54n5316nblkqcy8vr0000gq/T//RtmpqTTFas/downloaded_packages\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\t/var/folders/kw/rp8_72k54n5316nblkqcy8vr0000gq/T//RtmpqTTFas/downloaded_packages\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\t/var/folders/kw/rp8_72k54n5316nblkqcy8vr0000gq/T//RtmpqTTFas/downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in download.file(url, destfile, method, mode = \"wb\", ...):\n",
      "“downloaded length 0 != reported length 21722”\n",
      "Warning message in download.file(url, destfile, method, mode = \"wb\", ...):\n",
      "“URL 'https://cran.r-project.org/bin/macosx/el-capitan/contrib/3.6/tsne_0.1-3.tgz': status was 'Failure when receiving data from the peer'”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in download.file(url, destfile, method, mode = \"wb\", ...) : \n",
      "  download from 'https://cran.r-project.org/bin/macosx/el-capitan/contrib/3.6/tsne_0.1-3.tgz' failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in download.packages(pkgs, destdir = tmpd, available = available, :\n",
      "“download of package ‘tsne’ failed”\n",
      "also installing the dependencies ‘credentials’, ‘zip’, ‘gitcreds’, ‘gert’, ‘gh’, ‘rappdirs’\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The downloaded binary packages are in\n",
      "\t/var/folders/kw/rp8_72k54n5316nblkqcy8vr0000gq/T//RtmpqTTFas/downloaded_packages\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\t/var/folders/kw/rp8_72k54n5316nblkqcy8vr0000gq/T//RtmpqTTFas/downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"tidyverse\")\n",
    "install.packages(\"tidytext\")\n",
    "install.packages(\"magrittr\")\n",
    "install.packages(\"devtools\")\n",
    "install.packages(\"tsne\")\n",
    "install.packages(\"usethis\")\n",
    "install.packages(\"SnowballC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidytext)\n",
    "library(magrittr)\n",
    "library(devtools)\n",
    "library(tsne)\n",
    "library(usethis)\n",
    "library(SnowballC)\n",
    "# Hold off on running the line below until after you get to the next section \n",
    "library(wordVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading GitHub repo bmschmidt/wordVectors@HEAD\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlang  (0.4.8    -> 0.4.10  ) [CRAN]\n",
      "vctrs  (0.3.5    -> 0.3.6   ) [CRAN]\n",
      "pillar (1.4.6    -> 1.4.7   ) [CRAN]\n",
      "fansi  (0.4.1    -> 0.4.2   ) [CRAN]\n",
      "crayon (1.3.4    -> 1.4.0   ) [CRAN]\n",
      "cli    (2.1.0    -> 2.3.0   ) [CRAN]\n",
      "cpp11  (0.2.4    -> 0.2.6   ) [CRAN]\n",
      "BH     (1.72.0-3 -> 1.75.0-0) [CRAN]\n",
      "tibble (3.0.4    -> 3.0.6   ) [CRAN]\n",
      "hms    (0.5.3    -> 1.0.0   ) [CRAN]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing 10 packages: rlang, vctrs, pillar, fansi, crayon, cli, cpp11, BH, tibble, hms\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The downloaded binary packages are in\n",
      "\t/var/folders/kw/rp8_72k54n5316nblkqcy8vr0000gq/T//RtmpqTTFas/downloaded_packages\n",
      "\u001b[32m✔\u001b[39m  \u001b[38;5;247mchecking for file ‘/private/var/folders/kw/rp8_72k54n5316nblkqcy8vr0000gq/T/RtmpqTTFas/remoteseb081eef972c/bmschmidt-wordVectors-7f1914c/DESCRIPTION’\u001b[39m\u001b[36m\u001b[36m (3.1s)\u001b[36m\u001b[39m\n",
      "\u001b[38;5;247m─\u001b[39m\u001b[38;5;247m  \u001b[39m\u001b[38;5;247mpreparing ‘wordVectors’:\u001b[39m\u001b[36m\u001b[36m (687ms)\u001b[36m\u001b[39m\n",
      "\u001b[32m✔\u001b[39m  \u001b[38;5;247mchecking DESCRIPTION meta-information\u001b[39m\u001b[36m\u001b[39m\n",
      "\u001b[38;5;247m─\u001b[39m\u001b[38;5;247m  \u001b[39m\u001b[38;5;247mcleaning src\u001b[39m\u001b[36m\u001b[39m\n",
      "\u001b[38;5;247m─\u001b[39m\u001b[38;5;247m  \u001b[39m\u001b[38;5;247mchecking for LF line-endings in source and make files and shell scripts\u001b[39m\u001b[36m\u001b[36m (402ms)\u001b[36m\u001b[39m\n",
      "\u001b[38;5;247m─\u001b[39m\u001b[38;5;247m  \u001b[39m\u001b[38;5;247mchecking for empty or unneeded directories\u001b[39m\u001b[36m\u001b[39m\n",
      "\u001b[38;5;247m─\u001b[39m\u001b[38;5;247m  \u001b[39m\u001b[38;5;247mlooking to see if a ‘data/datalist’ file should be added\u001b[39m\u001b[36m\u001b[39m\n",
      "\u001b[38;5;247m─\u001b[39m\u001b[38;5;247m  \u001b[39m\u001b[38;5;247mbuilding ‘wordVectors_2.0.tar.gz’\u001b[39m\u001b[36m\u001b[39m\n",
      "   \n",
      "\r"
     ]
    }
   ],
   "source": [
    "devtools::install_github('bmschmidt/wordVectors', force=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next portion, please be sure to provide the correct file name. This path2file currently directs the code to the documents concerning Latin America during the Cold War. If you would rather analyze the European files, input \"data/frusEU\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data/frusLA/frus1945v09.txt\n",
      "\n",
      "data/frusLA/frus1946v11.txt\n",
      "\n",
      "data/frusLA/frus1947v08.txt\n",
      "\n",
      "data/frusLA/frus1948v09.txt\n",
      "\n",
      "data/frusLA/frus1949v02.txt\n",
      "\n",
      "data/frusLA/frus1950v02.txt\n",
      "\n",
      "data/frusLA/frus1951v02.txt\n",
      "\n",
      "data/frusLA/frus1952-54v04.txt\n",
      "\n",
      "data/frusLA/frus1955-57v06.txt\n",
      "\n",
      "data/frusLA/frus1955-57v07.txt\n",
      "\n",
      "data/frusLA/frus1958-60v05.txt\n",
      "\n",
      "data/frusLA/frus1958-60v06.txt\n",
      "\n",
      "data/frusLA/frus1961-63v10.txt\n",
      "\n",
      "data/frusLA/frus1961-63v12.txt\n",
      "\n",
      "data/frusLA/frus1964-68v31.txt\n",
      "\n",
      "data/frusLA/frus1964-68v32.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path2file <- \"data/frusLA\"\n",
    "fileList <- list.files(path2file,full.names = TRUE) \n",
    "\n",
    "readTextFiles <- function(file) { \n",
    "  message(file)\n",
    "  rawText = paste(scan(file, sep=\"\\n\",what=\"raw\",strip.white = TRUE))\n",
    "  output = tibble(filename=gsub(path2file,\"\",file),text=rawText) %>% \n",
    "    group_by(filename) %>% \n",
    "    summarise(text = paste(rawText, collapse = \" \"))\n",
    "  return(output)\n",
    "}\n",
    "\n",
    "combinedTexts <- tibble(filename=fileList) %>% \n",
    "  group_by(filename) %>% \n",
    "  do(readTextFiles(.$filename)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next chunk of code, users should  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to change the text in the first line to whatever you want to call your model file\n",
    "baseFile <- \"your_file_name\"\n",
    "w2vInput <- paste(\"data/\",baseFile,\".txt\", sep = \"\")\n",
    "w2vCleaned <- paste(\"data/\",baseFile,\"_cleaned.txt\", sep=\"\")\n",
    "w2vBin <- paste(\"data/\",baseFile,\".bin\", sep=\"\")\n",
    "combinedTexts$text %>% write_lines(w2vInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model is based on the parameters that are selected prior to running the code. Word embedding models allow you to “choose how expansive you want the explored space to be” (Schmidt 2015). Tuning the parameters results in a greater accuracy depending on the analysis you are intending to complete. In order to test the usage of terms in the corpus, I tested a large variety of parameters in order to determine the accuracy of each model. For each set of parameters, both the Latin American and European corpora were tested and were run through six iterations on each corpus. Ultimately, there were six sets of parameters and therefore 62 models created in total. A list of the models and their parameters can be found here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in prep_word2vec(origin = w2vInput, destination = w2vCleaned, lowercase = T, : could not find function \"prep_word2vec\"\n",
     "output_type": "error",
     "traceback": [
      "Error in prep_word2vec(origin = w2vInput, destination = w2vCleaned, lowercase = T, : could not find function \"prep_word2vec\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "THREADS <- 3\n",
    "\n",
    "prep_word2vec(origin=w2vInput,destination=w2vCleaned,lowercase=T,bundle_ngrams=1)\n",
    "\n",
    "if (!file.exists(w2vBin)) {\n",
    "  w2vModel <- train_word2vec(\n",
    "    w2vCleaned,\n",
    "    output_file=w2vBin,\n",
    "    vectors=500,\n",
    "    threads=THREADS,\n",
    "    window=10, iter=10, negative_samples=15\n",
    "  )\n",
    "} else {\n",
    "  w2vModel <- read.vectors(w2vBin)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you choose to read in an existing .bin file, please see the next chunk of code. For example, I have included the .bin files for both Latin America documents and European documents that I use in the later analysis. If you would like to explore those documents instead of waitinig for a model to run, either input \"LA6a.bin\" for the corpus on Latin America, or \"EU6a.bin\" for the European-related corpus. To use this code chunk, please remove the \"#\" before the code. \"#\" acts as a way to comment in the code, thus rendering the code inactive, if the user chooses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to read in an existing file. Please see the Github repository for various iterations of this model that you can feed in. Be sure to remove the hashtag to choose this option instead.\n",
    "  #w2vModel <- read.vectors(\"data/your_file_name.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an image of the model and some exploration into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel %>% plot(perplexity=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel %>% closest_to(\"girl\", 30) %>% View()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel %>% closest_to(~\"girl\"+\"woman\"+\"girls\"+\"women\", 20) %>% View()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My thoughts on how these clusters show interesting patters of foreign policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers <- 150\n",
    "clustering <- kmeans(w2vModel,centers=centers,iter.max = 40)\n",
    "\n",
    "sapply(sample(1:centers, 10), function(n) {\n",
    "  names(clustering$cluster[clustering$cluster==n][1:15])\n",
    "})\n",
    "```\n",
    "\n",
    "#Change \"name_of_your_query\" to a descriptive name that you want to give to your export file.\n",
    "w2vExport <-sapply(sample(1:centers,150),function(n) {\n",
    "  names(clustering$cluster[clustering$cluster==n][1:15])\n",
    "})\n",
    "\n",
    "write.csv(file=\"output/name_of_your_query.csv\", x=w2vExport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original code includes a short method for validating the model. I have included five more word pairs that are good representations of the corpus. These were instrumental in determining which iterations of the model were the strongest representations of the corpus. You can find the Latin American validation data [here] (https://drive.google.com/file/d/1k75cYaVa8hmUtyoxOsi_0HX_mNmldD4B/view?usp=sharing) and the Europen validation data [here](https://drive.google.com/file/d/1oFj3rbjzaCtJgTvN0LvwPfVmyxIigJuo/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list  = list.files(pattern=\"*.bin$\", recursive=TRUE)\n",
    "\n",
    "rownames <- c()\n",
    "\n",
    "data_frame <- data.frame()\n",
    "data = list(c(\"away\", \"off\"),\n",
    "            c(\"before\", \"after\"),\n",
    "            c(\"cause\", \"effects\"),\n",
    "            c(\"children\", \"parents\"),\n",
    "            c(\"come\", \"go\"),\n",
    "            c(\"day\", \"night\"),\n",
    "            c(\"first\", \"second\"),\n",
    "            c(\"good\", \"bad\"),\n",
    "            c(\"last\", \"first\"),\n",
    "            c(\"kind\", \"sort\"),\n",
    "            c(\"leave\", \"quit\"),\n",
    "            c(\"life\", \"death\"),\n",
    "            c(\"girl\", \"boy\"),\n",
    "            c(\"little\", \"small\"),\n",
    "            c(\"oil\", \"petroleum\"),\n",
    "            c(\"state\", \"department\"),\n",
    "            c(\"confidential\", \"secret\"),\n",
    "            c(\"east\", \"west\"),\n",
    "            c(\"aid\", \"assistance\"))\n",
    "\n",
    "\n",
    "data_list = list()\n",
    "\n",
    "for(fn in files_list) {\n",
    "  \n",
    "  wwp_model = read.vectors(fn)\n",
    "  sims <- c()\n",
    "  for(pairs in data)\n",
    "  {\n",
    "    vector1 <- c()\n",
    "    for(x in wwp_model[[pairs[1]]]) {\n",
    "      vector1 <- c(vector1, x)\n",
    "    }\n",
    "    \n",
    "    vector2 <- c()\n",
    "    for(x in wwp_model[[pairs[2]]]) {\n",
    "      vector2 <- c(vector2, x)\n",
    "    }\n",
    "    \n",
    "    sims <- c(sims, cosine(vector1, vector2))\n",
    "    f_name <- strsplit(fn, \"/\")[[1]][[2]]\n",
    "    data_list[[f_name]] <- sims\n",
    "  }\n",
    "  \n",
    "}\n",
    "\n",
    "for(pairs in data) {\n",
    "  rownames <- c(rownames, paste(pairs[1], pairs[2], sep=\"-\"))\n",
    "}\n",
    "\n",
    "results <- structure(data_list,\n",
    "                     class     = \"data.frame\",\n",
    "                     row.names = rownames\n",
    ")\n",
    "\n",
    "write.csv(file=\"output/model-test-results.csv\", x=results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
